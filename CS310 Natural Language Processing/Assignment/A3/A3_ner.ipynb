{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3 (part 2). Named Entity Recognition with Bi-LSTM\n",
    "\n",
    "**Total points**: 30 + 20 bonus points\n",
    "\n",
    "In this assignment, you will train a bidirectional LSTM model on the CoNLL2003 English named entity recognition task set and evaluate its performance.\n",
    "\n",
    "For the bonus questions, submit them as separate notebook files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch.nn as nn   \n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/train.txt'\n",
    "DEV_PATH = 'data/dev.txt'\n",
    "TEST_PATH = 'data/test.txt'\n",
    "EMBEDDINGS_PATH = 'data/glove.6B.100d.txt' \n",
    "# Download from https://nlp.stanford.edu/data/glove.6B.zip\n",
    "# It includes dimension 50, 100, 200, and 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### process data to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ner_data(path_to_file):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "    # for vocab\n",
    "    words = set()\n",
    "    tags = set()\n",
    "    total_labels = 0\n",
    "    with open(path_to_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # a complete sentence\n",
    "            if len(line)==0 or line.startswith('-DOCSTART') or line[0]==\"\\n\":\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                    labels.append(label)\n",
    "                    label = []\n",
    "                continue\n",
    "\n",
    "            splitted = line.split()\n",
    "            word = splitted[0].lower() # use lower case\n",
    "            entity = splitted[-1]\n",
    "            words.add(word)\n",
    "            tags.add(entity)\n",
    "            sentence.append(word)\n",
    "            label.append(entity)\n",
    "            total_labels += 1\n",
    "    print('total labels: ', total_labels)\n",
    "    return sentences, labels, words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total labels:  203621\n",
      "total labels:  51362\n",
      "total labels:  46435\n"
     ]
    }
   ],
   "source": [
    "train_sentences, train_labels, train_words, train_tags = read_ner_data(TRAIN_PATH)\n",
    "val_sentences, val_labels, val_words, val_tags = read_ner_data(DEV_PATH)\n",
    "test_sentences, test_labels, test_words, test_tags = read_ner_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14041, 3250, 3453)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'],\n",
       "  ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']),\n",
       " (['peter', 'blackburn'], ['B-PER', 'I-PER']),\n",
       " (['brussels', '1996-08-22'], ['B-LOC', 'O'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(train_sentences[:3], train_labels[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['plymouth', '2', 'preston', '1'], ['B-ORG', 'O', 'B-ORG', 'O']),\n",
       " (['division', 'three'], ['O', 'O']),\n",
       " (['swansea', '1', 'lincoln', '2'], ['B-ORG', 'O', 'B-ORG', 'O'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(train_sentences[-3:], train_labels[-3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that\n",
    "- Each sentence ends with token '.' and tag 'O'. Between sentences there is a blank line.\n",
    "- Same padding and packing pipeline as in the previous lab need be used for the NER data, too.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Build vocabularies for both words and labels (tags)\n",
    "\n",
    "Use *ALL* the data from train, dev, and test sets to build the vocabularies, for word and label (tag), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = train_words.union(val_words).union(test_words)\n",
    "tags = train_tags.union(val_tags).union(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "word2id = {}\n",
    "id = 0\n",
    "for word in words:\n",
    "    if word not in word2id:\n",
    "        word2id[word] = id\n",
    "        id += 1\n",
    "\n",
    "tag2id = {}\n",
    "tag2id['<PAD>'] = 0\n",
    "id = 1 # 0 for Pad\n",
    "for tag in tags:\n",
    "    if tag not in tag2id:\n",
    "        tag2id[tag] = id\n",
    "        id += 1\n",
    "\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "id2tag = {v: k for k, v in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26869,\n",
       " 10,\n",
       " dict_values([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " dict_keys(['<PAD>', 'I-ORG', 'I-MISC', 'B-MISC', 'O', 'B-PER', 'I-LOC', 'I-PER', 'B-LOC', 'B-ORG']))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id), len(tag2id), tag2id.values(), tag2id.keys()\n",
    "# 0 is for padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load word vectors from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100 # glove 100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read glove embeddings\n",
    "embedding_dict = {}\n",
    "with open(EMBEDDINGS_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_dict[word] = vector\n",
    "# ('word': w2v_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2id)\n",
    "embedding_matrix = torch.zeros(vocab_size, EMBEDDING_DIM)\n",
    "\n",
    "for word, i in word2id.items():\n",
    "    embedding_vector = embedding_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = torch.tensor(embedding_vector)\n",
    "    else:\n",
    "        embedding_matrix[i] = torch.rand(EMBEDDING_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
    "embedding.weight = nn.Parameter(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(26869, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data into indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_ids = [torch.tensor([word2id[word] for word in sentence]).to(device) for sentence in train_sentences]\n",
    "val_sentences_ids = [torch.tensor([word2id[word] for word in sentence]).to(device) for sentence in val_sentences]\n",
    "test_sentences_ids = [torch.tensor([word2id[word] for word in sentence]).to(device) for sentence in test_sentences]\n",
    "train_labels_ids = [torch.tensor([tag2id[tag] for tag in labels]).to(device) for labels in train_labels]\n",
    "val_labels_ids = [torch.tensor([tag2id[tag] for tag in labels]).to(device) for labels in val_labels]\n",
    "test_labels_ids = [torch.tensor([tag2id[tag] for tag in labels]).to(device) for labels in test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seq lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_seqs_len = [len(seq) for seq in val_sentences_ids]\n",
    "test_seqs_len = [len(seq) for seq in test_sentences_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 2, 35]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_seqs_len[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.'],\n",
       "  ['peter', 'blackburn'],\n",
       "  ['brussels', '1996-08-22']],\n",
       " [['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O'],\n",
       "  ['B-PER', 'I-PER'],\n",
       "  ['B-LOC', 'O']])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:3], train_labels[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad Val and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(data_seq_ids: list, vocab: dict):\n",
    "    ids_padded = nn.utils.rnn.pad_sequence(data_seq_ids, batch_first=True)\n",
    "    return ids_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, hidden_dim: int, output_size: int, embedding: nn.Embedding, num_layers: int = 1):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.word_embeddings = embedding\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_size) # for bilstm\n",
    "    \n",
    "\n",
    "    def forward(self, padded_seqs, seq_lens):\n",
    "        padded_embs = self.word_embeddings(padded_seqs)\n",
    "        packed_embs = nn.utils.rnn.pack_padded_sequence(padded_embs, seq_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out_packed, _ = self.lstm(packed_embs)\n",
    "        # self.lstm 的输出是 (batch_size, sequence_length, hidden_dim * num_directions)\n",
    "        out_unpacked, _ = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True) # [16, 40, 256])?\n",
    "        logits = self.fc(out_unpacked)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate batches (Padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(sentences: list[list], labels: list[list], batch_size: int):\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        seqs = sentences[i:i+batch_size]\n",
    "        tags = labels[i:i+batch_size]\n",
    "        # convert words and tags to ids\n",
    "        seqs_ids = [torch.tensor([word2id[word] for word in sentence]).to(device) for sentence in seqs]\n",
    "        tags_ids = [torch.tensor([tag2id[label] for label in labels]).to(device) for labels in tags]\n",
    "        seq_lens = torch.tensor([len(ids) for ids in seqs_ids])\n",
    "\n",
    "        padded_seqs = nn.utils.rnn.pad_sequence(seqs_ids, batch_first=True)\n",
    "        padded_tags = nn.utils.rnn.pad_sequence(tags_ids, batch_first=True)\n",
    "        \n",
    "        yield padded_seqs, padded_tags, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(batchify(train_sentences, train_labels, batch_size))\n",
    "# sentence, label, seq_lens for each batch (all are padded!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6453, 17485,  1947, 22389, 15618, 16350, 13995, 23654, 22991,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]),\n",
       " tensor([9, 4, 3, 4, 4, 4, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor(9))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0][0][0], batches[0][1][0], batches[0][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model:LSTMTagger, sentence_ids, label_ids,loss_function): # raw data ids\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        padded_seq_ids = pad_data(sentence_ids, word2id)\n",
    "        padded_label_ids = pad_data(label_ids, tag2id)\n",
    "        seq_lens = torch.tensor([len(ids) for ids in sentence_ids])\n",
    "        log_probs = model.forward(padded_seq_ids, seq_lens)\n",
    "        \n",
    "        predicted_labels = torch.argmax(log_probs, dim=2)\n",
    "        predicted_labels_flat = predicted_labels.view(-1)\n",
    "        true_labels_flat = padded_label_ids.view(-1)\n",
    "\n",
    "        # Ignore padding labels\n",
    "        non_zero_indices= (true_labels_flat != 0).nonzero().squeeze()\n",
    "        real_true_labels = true_labels_flat[non_zero_indices]\n",
    "        real_predicted_labels = predicted_labels_flat[non_zero_indices]\n",
    "\n",
    "        total += len(real_predicted_labels) # 51362\n",
    "        correct += torch.sum(real_predicted_labels == real_true_labels).item()\n",
    "        \n",
    "        # calculate f1\n",
    "        predicted_labels_np = real_predicted_labels.cpu().numpy() # first move to cpu\n",
    "        true_labels_np = real_true_labels.cpu().numpy()\n",
    "        f1 = f1_score(true_labels_np, predicted_labels_np, average='macro')\n",
    "        \n",
    "        loss = loss_function(log_probs.view(-1, log_probs.size(-1)), padded_label_ids.view(-1))\n",
    "\n",
    "    return correct/total, f1, loss.mean().item()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:LSTMTagger,\n",
    "          optimizer: optim.Optimizer,\n",
    "          loss_function,\n",
    "          train_seq: list,\n",
    "          train_labels: list,\n",
    "          batch_size: int,\n",
    "          padded_val_sentences_ids: list, # padded already\n",
    "          padded_val_labels_ids: list,\n",
    "          log_interval=20,\n",
    "          epochs: int=2):\n",
    "    \n",
    "    batches = list(batchify(train_seq, train_labels, batch_size))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (padded_seqs, padded_labels, seq_lens) in enumerate(batches): # sentences and labels are padded\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model.forward(padded_seqs, seq_lens)\n",
    "\n",
    "            # print(log_probs.shape) # 16, 40, 9\n",
    "            # print(padded_labels.shape) # 16,40\n",
    "            # 第一个维度的大小是 batch_size * sequence_length，第二个维度的大小是词汇表中词汇的数量（即 len(word2id)）\n",
    "            # 这样做的目的是为了将每个时间步的概率值按照每个词汇进行展开。\n",
    "            # targets_padded.view(-1) 展开成一个一维张量，其中包含了所有的目标标签。\n",
    "            loss = loss_function(log_probs.view(-1, log_probs.size(-1)), padded_labels.view(-1))\n",
    "\n",
    "            loss = loss.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                 print(\n",
    "                    \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                    \"| loss {:8.3f}\".format(\n",
    "                        epoch, i, len(batches), loss.item()\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        print(f'Epoch {epoch} Avg Loss: {total_loss/len(batches)}')\n",
    "        print(\"Validating on dev test: \")\n",
    "        acc, f1, loss = evaluate(model, padded_val_sentences_ids, padded_val_labels_ids, loss_function)\n",
    "        print(f'Epoch {epoch} Validation Loss: {loss} Accuracy: {acc} F1: {f1}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 128\n",
    "num_layers = 2\n",
    "epochs = 2\n",
    "learning_rate = 0.02\n",
    "batch_size = 128\n",
    "log_interval = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(tag2id), embedding).to(device)\n",
    "# if this fail, please rerun this, and below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMTagger(\n",
       "  (word_embeddings): Embedding(26869, 100)\n",
       "  (lstm): LSTM(100, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss(reduction='none', ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |    20/  110 batches | loss    0.134\n",
      "| epoch   0 |    40/  110 batches | loss    0.286\n",
      "| epoch   0 |    60/  110 batches | loss    0.133\n",
      "| epoch   0 |    80/  110 batches | loss    0.031\n",
      "| epoch   0 |   100/  110 batches | loss    0.034\n",
      "Epoch 0 Avg Loss: 0.09791958834975958\n",
      "Validating on dev test: \n",
      "Epoch 0 Validation Loss: 0.018458673730492592 Accuracy: 0.9647599392547019 F1: 0.8169451266879797\n",
      "| epoch   1 |    20/  110 batches | loss    0.048\n",
      "| epoch   1 |    40/  110 batches | loss    0.013\n",
      "| epoch   1 |    60/  110 batches | loss    0.042\n",
      "| epoch   1 |    80/  110 batches | loss    0.014\n",
      "| epoch   1 |   100/  110 batches | loss    0.009\n",
      "Epoch 1 Avg Loss: 0.024646524801342325\n",
      "Validating on dev test: \n",
      "Epoch 1 Validation Loss: 0.01679503545165062 Accuracy: 0.968517581091079 F1: 0.8470849547301502\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, loss_function, train_sentences, train_labels, batch_size, val_sentences_ids, val_labels_ids,log_interval, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac, f1, loss = evaluate(model, test_sentences_ids, test_labels_ids, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.016579529270529747 Accuracy: 0.9566921503176483 F1: 0.80884299584986\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Loss: {loss} Accuracy: {ac} F1: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path = os.getcwd()\n",
    "torch.save(model.state_dict(), cur_path + '/ner_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
