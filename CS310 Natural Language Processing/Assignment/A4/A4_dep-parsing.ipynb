{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 4. Dependency Parsing\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "In this assignment, you will train feed-forward neural network-based dependency parser and evaluate its performance on the provided treebank dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference: \n",
    "https://github.com/lmxy0212/NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from dep_utils import conll_reader, DependencyTree, DependencyEdge\n",
    "import copy\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Data and Generate Training Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train.conll:\n",
      "39832 trees read.\n",
      "In dev.conll:\n",
      "1700 trees read.\n",
      "In test.conll:\n",
      "2416 trees read.\n"
     ]
    }
   ],
   "source": [
    "print('In train.conll:')\n",
    "with open('data/train.conll') as f:\n",
    "    train_trees = list(conll_reader(f))\n",
    "print(f'{len(train_trees)} trees read.')\n",
    "\n",
    "print('In dev.conll:')\n",
    "with open('data/dev.conll') as f:\n",
    "    dev_trees = list(conll_reader(f))\n",
    "print(f'{len(dev_trees)} trees read.')\n",
    "\n",
    "print('In test.conll:')\n",
    "with open('data/test.conll') as f:\n",
    "    test_trees = list(conll_reader(f))\n",
    "print(f'{len(test_trees)} trees read.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The top of stack is `stack[-1]`\n",
    "- The front of buffer is `buffer[-1]`\n",
    "- `deps` represents the currently found dependencies\n",
    "  - It is a list of `(parent, child, relation)` triples, where `parent` and `child` are integer IDs and `relation` is a string (the dependency label).\n",
    "- The `shift` methods moves the front of the buffer to the top of the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, sentence=[]):\n",
    "        self.stack = []\n",
    "        self.buffer = []\n",
    "        if sentence:\n",
    "            self.buffer = list(reversed(sentence))\n",
    "        self.deps = set()\n",
    "\n",
    "    def shift(self):\n",
    "        assert len(self.buffer) > 0\n",
    "        self.stack.append(self.buffer.pop())\n",
    "\n",
    "    def left_arc(self, label):\n",
    "        assert len(self.stack) >= 2\n",
    "        self.deps.add((self.stack[-1], self.stack[-2], label))\n",
    "        self.stack.pop(-2)\n",
    "\n",
    "    def right_arc(self, label):\n",
    "        assert len(self.stack) >= 2\n",
    "        self.deps.add((self.stack[-2], self.stack[-1], label))\n",
    "        self.stack.pop(-1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"({},{},{})\".format(self.stack, self.buffer, self.deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get training data from a dependency tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return type of this function is a list of two-elements tuples:\n",
    "- Tuple[0] is a `State` object, deepcopied from the initial state\n",
    "- Tuple[1] is a a tuple of `(action, relation)` where `action` is from {\"shift\", \"left_arc\", \"right_arc\"} and `relation` is the specific dependency relation.\n",
    "\n",
    "- If $s_1 \\rightarrow s_2$ exists in `deprels`, then `left_arc` is performed.\n",
    "- If $s_2 \\rightarrow s_1$ exists in `deprels`, **AND** all rules with $s_1$ as the head have already been assigned, then `right_arc` is performed.\n",
    "- Perform `shift` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RootDummy(object):\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.id = 0\n",
    "        self.deprel = None\n",
    "    def __repr__(self):\n",
    "        return \"<ROOT>\"\n",
    "\n",
    "\n",
    "def get_training_instances(dep_tree: DependencyTree) -> List[Tuple[State, Tuple[str, str]]]:\n",
    "    deprels = dep_tree.deprels\n",
    "\n",
    "    word_ids = list(deprels.keys())\n",
    "    state = State(word_ids)\n",
    "    state.stack.append(0) # ROOT\n",
    "\n",
    "    childcount = defaultdict(int)\n",
    "    for _, rel in deprels.items():\n",
    "        childcount[rel.head] += 1\n",
    "\n",
    "    seq = []\n",
    "    while len(state.buffer) > 0 or len(state.stack) > 1:\n",
    "        if state.stack[-1] == 0:\n",
    "            seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "            state.shift()\n",
    "            continue\n",
    "        \n",
    "        stack_top1 = deprels[state.stack[-1]]\n",
    "        if state.stack[-2] == 0:\n",
    "            stack_top2 = RootDummy()\n",
    "        else:\n",
    "            stack_top2 = deprels[state.stack[-2]]\n",
    "\n",
    "        # Decide transition action\n",
    "        ### START YOUR CODE ###\n",
    "        try:\n",
    "            if stack_top2.head == stack_top1.id : # Left-Arc, top1 -> top2\n",
    "                childcount[stack_top1.id] -= 1\n",
    "                seq.append((copy.deepcopy(state), (\"left_arc\", stack_top2.deprel)))\n",
    "                state.left_arc(stack_top2.deprel)\n",
    "            elif stack_top1.head == stack_top2.id and childcount[stack_top1.id] == 0: # Right-Arc, top2 -> top1\n",
    "                childcount[stack_top2.id] -= 1\n",
    "                seq.append((copy.deepcopy(state), (\"right_arc\", stack_top1.deprel)))\n",
    "                state.right_arc(stack_top1.deprel)\n",
    "            else: # Shift\n",
    "                seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "                state.shift()\n",
    "        except:\n",
    "            return seq\n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "pos2id = {}\n",
    "def get_vocabs(trees: List[DependencyTree]):\n",
    "    for tree in trees:\n",
    "        word = tree.words()\n",
    "        pos = tree.pos()\n",
    "        for w in word:\n",
    "            if w is None:\n",
    "                continue\n",
    "            if w not in word2id:\n",
    "                word2id[w] = len(word2id)\n",
    "        for p in pos:\n",
    "            if p is None:\n",
    "                continue\n",
    "            if p not in pos2id:\n",
    "                pos2id[p] = len(pos2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vocab: 46350 words\n",
      "pos_vocab: 47 pos tags\n"
     ]
    }
   ],
   "source": [
    "get_vocabs(train_trees)\n",
    "get_vocabs(dev_trees)\n",
    "get_vocabs(test_trees)\n",
    "\n",
    "word2id['<NULL>'] = len(word2id)\n",
    "pos2id['<NULL>'] = len(pos2id)\n",
    "word2id['<ROOT>'] = len(word2id)\n",
    "pos2id['<ROOT>'] = len(pos2id)\n",
    "\n",
    "\n",
    "print(f'word_vocab: {len(word2id)} words')\n",
    "print(f'pos_vocab: {len(pos2id)} pos tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Action Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number fo unique relations: 39\n",
      "dict_keys(['case', 'det', 'compound', 'nummod', 'nmod', 'punct', 'nmod:poss', 'amod', 'nsubj', 'dep', 'dobj', 'cc', 'conj', 'nsubjpass', 'acl', 'auxpass', 'advmod', 'root', 'ccomp', 'mark', 'xcomp', 'nmod:tmod', 'appos', 'nmod:npmod', 'aux', 'cop', 'neg', 'acl:relcl', 'advcl', 'mwe', 'det:predet', 'csubj', 'parataxis', 'compound:prt', 'iobj', 'expl', 'cc:preconj', 'discourse', 'csubjpass'])\n"
     ]
    }
   ],
   "source": [
    "rel_vocab = {}\n",
    "\n",
    "for t in train_trees+dev_trees+test_trees:\n",
    "    for e in t.deprels.values():\n",
    "        if e.deprel not in rel_vocab:\n",
    "            rel_vocab[e.deprel] = len(rel_vocab)\n",
    "\n",
    "# Test results\n",
    "print('Total number fo unique relations:', len(rel_vocab))\n",
    "print(rel_vocab.keys())\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# Total number fo unique relations: 39\n",
    "# {'nummod', 'root', 'nmod:tmod', 'nmod', 'punct', 'expl', 'auxpass', 'neg', 'nsubjpass', 'appos' ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action vocab\n",
    "action2id = {}\n",
    "action2id[('shift',None)] = len(action2id)\n",
    "for rel in rel_vocab.keys():\n",
    "    if rel != 'root':\n",
    "        action2id[(\"left_arc\", rel)] = len(action2id)\n",
    "        action2id[(\"right_arc\", rel)] = len(action2id)\n",
    "action2id[(\"right_arc\", 'root')] = len(action2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(action2id) # (39-1)*2 + 1(right_arc, root) + 1(shift, none) = 78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For actual training step, you need to post-process the data to convert each relation tuple to an integer index. \n",
    "- We have 39 unique dependency relations in the data, including `ROOT`. Considering `ROOT` only appears as the head in a `right_arc` action, we have $(39-1)\\times 2 + 1 = 77$ possible actions in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs:     $ùëí(ùë†_2)‚®Åùëí(ùë†_1)‚®Åùëí(ùë†_0)‚®Åùëí(ùëè_0)‚®Åùëí(ùëè_1)‚®Åùëí(ùëè_2)‚®Åùëí(ùë°ùë†_2)‚®Åùëí(ùë°ùë†_1)‚®Åùëí(ùë°ùë†_0)‚®Åùëí(ùë°ùëè_0)‚®Åùëí(ùë°ùëè_1)‚®Åùëí(ùë°ùëè_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor():\n",
    "    def __init__(self):\n",
    "        print('FeatureExtractor')\n",
    "\n",
    "    def get_input_representation(self, words, pos, state):\n",
    "        # (s2, s1, s0, b0, b1, b2, ts2, ts1, ts0, tb0, tb1, tb2)\n",
    "        input = []\n",
    "        for s in range(-3, 0): # top 3 words on the stack\n",
    "            if abs(s) <= len(state.stack):\n",
    "                sw_id = state.stack[s]\n",
    "                if sw_id == 0: # None, make it ROOT\n",
    "                    input.append(word2id['<ROOT>'])\n",
    "                else:\n",
    "                    input.append(word2id[words[sw_id]])\n",
    "            else:\n",
    "                input.append(word2id['<NULL>'])\n",
    "\n",
    "        for b in range(-1, -4, -1): # top 3 words on the buffer\n",
    "            if abs(b) <= len(state.buffer):\n",
    "                bw_id = state.buffer[b]\n",
    "                input.append(word2id[words[bw_id]])\n",
    "            else:\n",
    "                input.append(word2id['<NULL>'])\n",
    "\n",
    "        # pos\n",
    "        for i in range(-3, 0):\n",
    "            if abs(i) <= len(state.stack):\n",
    "                sw_id = state.stack[i]\n",
    "                if sw_id == 0:\n",
    "                    input.append(pos2id['<ROOT>'])\n",
    "                else:\n",
    "                    input.append(pos2id[pos[sw_id]])\n",
    "            else:\n",
    "                input.append(pos2id['<NULL>'])\n",
    "\n",
    "        for i in range(-1, -4, -1):\n",
    "            if abs(i) <= len(state.buffer):\n",
    "                bw_id = state.buffer[i]\n",
    "                input.append(pos2id[pos[bw_id]])\n",
    "            else:\n",
    "                input.append(pos2id['<NULL>'])\n",
    "                \n",
    "        # print(input)\n",
    "        return torch.LongTensor(input).to(device) \n",
    "\n",
    "    def get_output_representation(self, action):\n",
    "        return torch.tensor(action2id[action], dtype=torch.long).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor\n"
     ]
    }
   ],
   "source": [
    "# Test the FeatureExtractor\n",
    "dt = train_trees[23]\n",
    "fe = FeatureExtractor()\n",
    "seq = get_training_instances(dt)\n",
    "inputs = [] \n",
    "outputs = []\n",
    "for i, (state, action) in enumerate(seq):\n",
    "    words = dt.words()\n",
    "    pos = dt.pos()\n",
    "    input = fe.get_input_representation(words, pos, state)\n",
    "    output = fe.get_output_representation(action)\n",
    "    inputs.append(input)\n",
    "    outputs.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 97)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs), len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([46348, 46348, 46349,   312,    19,   151,    45,    45,    46,    29,\n",
       "              1,    10], device='cuda:1'),\n",
       "  tensor([46348, 46349,   312,    19,   151,   259,    45,    46,    29,     1,\n",
       "             10,    29], device='cuda:1'),\n",
       "  tensor([46349,   312,    19,   151,   259,   291,    46,    29,     1,    10,\n",
       "             29,    25], device='cuda:1')],\n",
       " [tensor(0, device='cuda:1'),\n",
       "  tensor(0, device='cuda:1'),\n",
       "  tensor(0, device='cuda:1')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:3], outputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(dep_trees: List[DependencyTree], word_vocab: dict, pos_vocab: dict, action_vocab, extractor):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i, tree in enumerate(dep_trees):\n",
    "        words = tree.words()\n",
    "        pos = tree.pos()\n",
    "        # i = 23 Âç°Âú®get_training_instances\n",
    "        instances = get_training_instances(tree)\n",
    "        if i % 1000 == 0:\n",
    "            print(f'{i}/{len(dep_trees)}')\n",
    "        for state, action in instances:\n",
    "            # convert to torch tensor\n",
    "            inputs.append(extractor.get_input_representation(words, pos, state))\n",
    "            outputs.append(extractor.get_output_representation(action))\n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('train_data.pt') and os.path.exists('train_label.pt'):\n",
    "    train_data = torch.load('train_data.pt')\n",
    "    train_label = torch.load('train_label.pt')\n",
    "else:\n",
    "    train_data, train_label = process(train_trees, word2id, pos2id, action2id, FeatureExtractor())\n",
    "    train_data = torch.stack(train_data)\n",
    "    train_label = torch.stack(train_label)\n",
    "    torch.save(train_data, 'train_data.pt')\n",
    "    torch.save(train_label, 'train_label.pt')\n",
    "    # this may take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.to(device)\n",
    "train_label = train_label.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1899390, 1899390, torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(train_label), type(train_data), type(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([46348, 46348, 46349,     0,     1,     2,    45,    45,    46,     0,\n",
       "             1,     2], device='cuda:1'),\n",
       " tensor(0, device='cuda:1'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0], train_label[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46350, 47, 78)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dim = len(word2id)\n",
    "pos_dim = len(pos2id)\n",
    "feature_len = len(train_data[0])\n",
    "out_dim = len(action2id)\n",
    "emb_dim = 50\n",
    "hidden_dim = 100\n",
    "word_dim, pos_dim, out_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNOracle(nn.Module):\n",
    "    def __init__(self,input_len, word_dim, pos_dim, emb_dim, out_dim, hidden_dim=100):\n",
    "        super(NNOracle, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(num_embeddings=word_dim, embedding_dim=emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=pos_dim, embedding_dim=emb_dim)\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(emb_dim * input_len, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Linear(10, out_dim)\n",
    "        # softmax layer is calculated outside\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.word_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.pos_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        for layer in self.hidden_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.uniform_(-initrange, initrange)\n",
    "                layer.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        word_tensor, pos_tensor = torch.chunk(x, 2, dim=1)\n",
    "        word_emb = self.word_embedding(word_tensor)\n",
    "        pos_emb = self.pos_embedding(pos_tensor)\n",
    "        x = torch.cat((word_emb, pos_emb), dim=1)\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parser Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(object): \n",
    "\n",
    "    def __init__(self, model: NNOracle):\n",
    "        self.model = model\n",
    "        self.extractor = FeatureExtractor()\n",
    "        self.id2action = {v: k for k, v in action2id.items()}\n",
    "\n",
    "    def parse_sentence(self, words, pos):\n",
    "        state = State(range(1, len(words)))\n",
    "        state.stack.append(0) # ROOT\n",
    "\n",
    "        while len(state.buffer) > 0 or len(state.stack) > 1:\n",
    "            model_input = self.extractor.get_input_representation(words, pos, state)\n",
    "            model_out = self.model.forward(model_input.unsqueeze(0))\n",
    "            probs = torch.softmax(model_out, dim=1)\n",
    "            sorted_indices = torch.argsort(probs, dim=1, descending=True)\n",
    "            sorted_indices = torch.squeeze(sorted_indices)\n",
    "            for i in range(0, len(sorted_indices)): # might have illegal actions\n",
    "                move, rel = self.id2action[sorted_indices[i].item()]\n",
    "                if move == 'shift' and len(state.buffer) > 0:\n",
    "                    state.shift()\n",
    "                    break\n",
    "                elif len(state.stack) >= 2:\n",
    "                    if move == 'left_arc' and state.stack[-2] != 0 and rel != 'root':\n",
    "                        state.left_arc(rel)\n",
    "                        break\n",
    "                    if move == 'right_arc':\n",
    "                        state.right_arc(rel)\n",
    "                        break\n",
    "\n",
    "        result = DependencyTree()\n",
    "        for h, c, r in state.deps: # head, child(dependent), relation\n",
    "            result.add_deprel(DependencyEdge(c, words[c], pos[c], h, r))\n",
    "        return result \n",
    "    \n",
    "    # compare the predicted tree with the reference tree\n",
    "    def compare_tree(self, ref_tree: DependencyTree, prediction: DependencyTree):\n",
    "        # unlabeled does not care about the relation\n",
    "        target_unlabeled = set((d.id,d.head) for d in ref_tree.deprels.values())\n",
    "        target_labeled = set((d.id,d.head,d.deprel) for d in ref_tree.deprels.values())\n",
    "        predict_unlabeled = set((d.id,d.head) for d in prediction.deprels.values())\n",
    "        predict_labeled = set((d.id,d.head,d.deprel) for d in prediction.deprels.values())\n",
    "\n",
    "        labeled_correct = len(predict_labeled.intersection(target_labeled))\n",
    "        unlabeled_correct = len(predict_unlabeled.intersection(target_unlabeled))\n",
    "        num_words = len(predict_labeled)\n",
    "        return labeled_correct, unlabeled_correct, num_words \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dep_trees: List[DependencyTree], parser: Parser):\n",
    "    total_labeled_correct = 0\n",
    "    total_unlabeled_correct = 0\n",
    "    total_words = 0\n",
    "    count = 0 \n",
    "    print(\"Evaluating.\")\n",
    "    for dtree in dep_trees:\n",
    "        words = dtree.words()\n",
    "        pos = dtree.pos()\n",
    "        prediction = parser.parse_sentence(words, pos)\n",
    "        labeled_correct, unlabeled_correct, num_words = parser.compare_tree(dtree, prediction)\n",
    "        total_labeled_correct += labeled_correct\n",
    "        total_unlabeled_correct += unlabeled_correct\n",
    "        total_words += num_words\n",
    "        count += 1 \n",
    "        if count % 200 == 0:\n",
    "            print(f'{count}/{len(dep_trees)}')\n",
    "\n",
    "    las = total_labeled_correct / float(total_words)\n",
    "    uas = total_unlabeled_correct / float(total_words)\n",
    "\n",
    "    print(f\"{len(dep_trees)} sentences.\\n\")\n",
    "    print(f\"Labeled Attachment Score: {las}\\n\")\n",
    "    print(f\"Unlabeled Attachment Score: {uas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_demo = NNOracle(input_len=feature_len, word_dim=word_dim, pos_dim=pos_dim, emb_dim=emb_dim, out_dim=out_dim, hidden_dim=hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor\n",
      "Evaluating.\n",
      "50 sentences.\n",
      "\n",
      "Labeled Attachment Score: 0.0\n",
      "\n",
      "Unlabeled Attachment Score: 0.0425531914893617\n"
     ]
    }
   ],
   "source": [
    "# test evaluate\n",
    "tree = dev_trees[:50]\n",
    "parser = Parser(model_demo)\n",
    "evaluate(tree, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNOracle(input_len=feature_len, word_dim=word_dim, pos_dim=pos_dim, emb_dim=emb_dim, out_dim=out_dim, hidden_dim=hidden_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: NNOracle,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        loss_function,\n",
    "        train_dataloader: DataLoader,\n",
    "        log_interval=500,\n",
    "        epochs: int = 3):\n",
    "    \n",
    "    model.train()\n",
    "    dev_parser = Parser(model)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        epoch_loss = 0\n",
    "        for id, (batch_data, batch_label) in enumerate(tqdm(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(batch_data)\n",
    "            output = output.cpu()\n",
    "            batch_label = batch_label.cpu()\n",
    "            # output:[batch_size, num_classes]\n",
    "            # label: [batch_size]\n",
    "            loss = loss_function(output, batch_label)\n",
    "            total_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.2) # Èò≤Ê≠¢Ê¢ØÂ∫¶ÁàÜÁÇ∏\n",
    "            optimizer.step()\n",
    "\n",
    "            if id % log_interval == 0 and id > 0:\n",
    "                print(\n",
    "                    \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                    \"| loss {:8.4f}\".format(\n",
    "                        epoch, id, len(train_dataloader), loss\n",
    "                    )\n",
    "                )\n",
    "                total_loss = 0\n",
    "\n",
    "        print(f'Epoch {epoch}, loss: {epoch_loss/len(train_dataloader)}')\n",
    "        print('--'*20)\n",
    "        evaluate(dev_trees, dev_parser)\n",
    "        print('--'*20)\n",
    "        epoch_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DepDataset(train_data, train_label)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 2025/7420 [00:09<00:25, 208.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |  2000/ 7420 batches | loss   0.5489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 4029/7420 [00:18<00:13, 246.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |  4000/ 7420 batches | loss   0.5127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 6040/7420 [00:26<00:05, 245.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 |  6000/ 7420 batches | loss   0.4861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7420/7420 [00:32<00:00, 227.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.5925818628177167\n",
      "----------------------------------------\n",
      "Evaluating.\n",
      "200/1700\n",
      "400/1700\n",
      "600/1700\n",
      "800/1700\n",
      "1000/1700\n",
      "1200/1700\n",
      "1400/1700\n",
      "1600/1700\n",
      "1700 sentences.\n",
      "\n",
      "Labeled Attachment Score: 0.7005508886506967\n",
      "\n",
      "Unlabeled Attachment Score: 0.7682777874716454\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 2038/7420 [00:09<00:21, 245.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  2000/ 7420 batches | loss   0.3893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 4038/7420 [00:17<00:13, 249.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  4000/ 7420 batches | loss   0.4215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 6038/7420 [00:25<00:05, 248.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  6000/ 7420 batches | loss   0.3699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7420/7420 [00:31<00:00, 233.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 0.339244027463895\n",
      "----------------------------------------\n",
      "Evaluating.\n",
      "200/1700\n",
      "400/1700\n",
      "600/1700\n",
      "800/1700\n",
      "1000/1700\n",
      "1200/1700\n",
      "1400/1700\n",
      "1600/1700\n",
      "1700 sentences.\n",
      "\n",
      "Labeled Attachment Score: 0.7326819054266271\n",
      "\n",
      "Unlabeled Attachment Score: 0.794301667622205\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, train_dataloader, epochs=epochs, log_interval=2000)\n",
    "torch.save(model, 'dep_model_nn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor\n",
      "Evaluating.\n",
      "200/2416\n",
      "400/2416\n",
      "600/2416\n",
      "800/2416\n",
      "1000/2416\n",
      "1200/2416\n",
      "1400/2416\n",
      "1600/2416\n",
      "1800/2416\n",
      "2000/2416\n",
      "2200/2416\n",
      "2400/2416\n",
      "2416 sentences.\n",
      "\n",
      "Labeled Attachment Score: 0.7357631783219251\n",
      "\n",
      "Unlabeled Attachment Score: 0.7955331310422694\n"
     ]
    }
   ],
   "source": [
    "test_model = torch.load('dep_model_nn.pt')\n",
    "parser = Parser(test_model)\n",
    "evaluate(test_trees, parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 1: Arc Eager Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "\n",
    "- http://fancyerii.github.io/books/depparser/\n",
    "- https://aclanthology.org/C12-1059/\n",
    "- https://direct.mit.edu/coli/article/40/2/259/1466/Arc-Eager-Parsing-with-the-Tree-Constraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EagerState(object):\n",
    "    def __init__(self, sentence=[]):\n",
    "        self.stack = []\n",
    "        self.buffer = []\n",
    "        if sentence:\n",
    "            self.buffer = list(reversed(sentence))\n",
    "        self.deps = set()\n",
    "\n",
    "    def shift(self):\n",
    "        assert len(self.buffer) > 0\n",
    "        self.stack.append(self.buffer.pop())\n",
    "\n",
    "    def left_arc(self, label):\n",
    "        assert len(self.stack) > 0 and len(self.buffer) > 0\n",
    "        self.deps.add((self.buffer[-1], self.stack[-1], label))\n",
    "        self.stack.pop()\n",
    "\n",
    "    def right_arc(self, label):\n",
    "        assert len(self.stack) > 0 and len(self.buffer) > 0\n",
    "        self.deps.add((self.stack[-1], self.buffer[-1], label))\n",
    "        self.stack.append(self.buffer.pop())\n",
    "    \n",
    "    def reduce(self):\n",
    "        assert len(self.stack) > 0\n",
    "        self.stack.pop()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"({},{},{})\".format(self.stack, self.buffer, self.deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eager_training_instances(dep_tree: DependencyTree) -> List[Tuple[EagerState, Tuple[str, str]]]:\n",
    "    deprels = dep_tree.deprels\n",
    "\n",
    "    word_ids = list(deprels.keys())\n",
    "    state = EagerState(word_ids)\n",
    "    state.stack.append(0) # ROOT\n",
    "\n",
    "    childcount = defaultdict(int)\n",
    "    for _, rel in deprels.items():\n",
    "        childcount[rel.head] += 1\n",
    "\n",
    "    seq = []\n",
    "    # a terminal configuration is any configuration with an empty buffer\n",
    "    while len(state.buffer) > 0:\n",
    "        if state.stack[-1] == 0:\n",
    "            seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "            state.shift()\n",
    "            continue\n",
    "        \n",
    "        stack_top = deprels[state.stack[-1]]\n",
    "        buffer_top = deprels[state.buffer[-1]]\n",
    "\n",
    "        try:\n",
    "            if stack_top.head == buffer_top.id:\n",
    "                childcount[buffer_top.id] -= 1\n",
    "                seq.append((copy.deepcopy(state), (\"left_arc\", stack_top.deprel)))\n",
    "                state.left_arc(stack_top.deprel)\n",
    "            elif buffer_top.head == stack_top.id:\n",
    "                childcount[stack_top.id] -= 1\n",
    "                seq.append((copy.deepcopy(state), (\"right_arc\", buffer_top.deprel)))\n",
    "                state.right_arc(buffer_top.deprel)\n",
    "            elif childcount[stack_top.id] == 0 and stack_top.id in [d[1] for d in state.deps]:\n",
    "                seq.append((copy.deepcopy(state), (\"reduce\", None)))\n",
    "                state.reduce()\n",
    "            else:\n",
    "                seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "                state.shift()\n",
    "        except:\n",
    "            return seq\n",
    "\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with Arc Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toy_data_standard length: 90\n",
      "[(([0],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1],set()),\n",
      "  ('shift', None)),\n",
      " (([0, 1],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2],set()),\n",
      "  ('shift', None)),\n",
      " (([0, 1, 2],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3],set()),\n",
      "  ('left_arc', 'det')),\n",
      " (([0, 2],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3],{(2, 1, 'det')}),\n",
      "  ('shift', None)),\n",
      " (([0, 2, 3],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4],{(2, 1, 'det')}),\n",
      "  ('right_arc', 'punct')),\n",
      " (([0, 2],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4],{(2, 1, 'det'), (2, 3, 'punct')}),\n",
      "  ('shift', None)),\n",
      " (([0, 2, 4],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5],{(2, 1, 'det'), (2, 3, 'punct')}),\n",
      "  ('shift', None)),\n",
      " (([0, 2, 4, 5],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6],{(2, 1, 'det'), (2, 3, 'punct')}),\n",
      "  ('left_arc', 'nmod:poss')),\n",
      " (([0, 2, 5],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6],{(2, 1, 'det'), (5, 4, 'nmod:poss'), (2, 3, 'punct')}),\n",
      "  ('shift', None)),\n",
      " (([0, 2, 5, 6],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7],{(2, 1, 'det'), (5, 4, 'nmod:poss'), (2, 3, 'punct')}),\n",
      "  ('left_arc', 'nsubj'))]\n"
     ]
    }
   ],
   "source": [
    "toy_data_standard = get_training_instances(dev_trees[1])\n",
    "print('toy_data_standard length:', len(toy_data_standard))\n",
    "pprint(toy_data_standard[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toy_data_eager length: 88\n",
      "[(([0],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1],set()),\n",
      "  ('shift', None)),\n",
      " (([0, 1],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2],set()),\n",
      "  ('left_arc', 'det')),\n",
      " (([0],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2],{(2, 1, 'det')}),\n",
      "  ('shift', None)),\n",
      " (([0, 2],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3],{(2, 1, 'det')}),\n",
      "  ('right_arc', 'punct')),\n",
      " (([0, 2, 3],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4],{(2, 1, 'det'), (2, 3, 'punct')}),\n",
      "  ('reduce', None)),\n",
      " (([0, 2],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4],{(2, 1, 'det'), (2, 3, 'punct')}),\n",
      "  ('shift', None)),\n",
      " (([0, 2, 4],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5],{(2, 1, 'det'), (2, 3, 'punct')}),\n",
      "  ('left_arc', 'nmod:poss')),\n",
      " (([0, 2],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5],{(2, 1, 'det'), (5, 4, 'nmod:poss'), (2, 3, 'punct')}),\n",
      "  ('shift', None)),\n",
      " (([0, 2, 5],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6],{(2, 1, 'det'), (5, 4, 'nmod:poss'), (2, 3, 'punct')}),\n",
      "  ('left_arc', 'nsubj')),\n",
      " (([0, 2],[45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6],{(2, 1, 'det'), (5, 4, 'nmod:poss'), (2, 3, 'punct'), (6, 5, 'nsubj')}),\n",
      "  ('right_arc', 'acl:relcl'))]\n"
     ]
    }
   ],
   "source": [
    "toy_data_eager = get_eager_training_instances(dev_trees[1])\n",
    "print('toy_data_eager length:', len(toy_data_eager))\n",
    "pprint(toy_data_eager[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 2: Bi-LSTM-based Encoder\n",
    "Please check the notebook `A4_lstm.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
