{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 5 (part 2): Pretraining BERT with on a Full Dataset\n",
    "\n",
    "You should re-use the code from A5_bert_toy.ipynb. For clarity, you are suggested to put the code for model definition in a separate file, e.g., model.py, and import it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import random\n",
    "from typing import List, Dict\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bert import BERT\n",
    "from transformers import BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './train.txt'\n",
    "test_raw_path = './test.raw.txt'\n",
    "test_pair_path = './test.pairs.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['北冥有鱼，其名为鲲。\\n', '鲲之大，不知其几千里也。\\n', '化而为鸟，其名为鹏。\\n'],\n",
       " ['其大本臃肿而不中绳墨，其小枝卷曲而不中规矩。\\n', '立之涂，匠者不顾。\\n', '今子之言，大而无用，众所同去也。\\n'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(train_path, 'r') as f:\n",
    "    train_text = f.readlines()\n",
    "with open(test_raw_path, 'r') as f:\n",
    "    test_raw_text = f.readlines()\n",
    "    \n",
    "train_text[:3], test_raw_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'壶子曰：乡吾示之以未始出吾宗。'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text = [re.sub(r'[\\n\\s]', '', x) for x in train_text] # remove \\n and space\n",
    "test_raw_text = [re.sub(r'[\\n\\s]', '', x) for x in test_raw_text] # remove \\n and space\n",
    "\n",
    "train_text[-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['竟', '婴', '口'], 1525)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_types = set(list(\"\".join(train_text+test_raw_text)))\n",
    "word_types = list(word_types)\n",
    "word_types[:3], len(word_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1529"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the special tokens to the vocabulary\n",
    "word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, w in enumerate(word_types):\n",
    "    word2id[w] = i + 4\n",
    "id2word = {i: w for i, w in enumerate(word2id)}\n",
    "VOCAB_SIZE = len(word2id)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[111, 363, 271, 345, 495, 787, 498, 564, 378, 24],\n",
       " [378, 105, 929, 495, 905, 505, 787, 1319, 679, 602, 1343, 24],\n",
       " [1215, 1473, 564, 650, 495, 787, 498, 564, 1403, 24]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_list = [[word2id[w] for w in s] for s in train_text]\n",
    "tokens_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max len should be the max ([CLS], sen1, [SEP], sen2, [SEP])\n",
    "MAX_LEN = 2 * max([len(x) for x in train_text]) + 3 # [CLS], [MASK], [SEP]\n",
    "MAX_PRED = round(MAX_LEN * 0.15)\n",
    "\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(tokens_list: List[List[int]], batch_size: int, word_to_id: Dict):\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    # [input_ids, segment_ids, masked_tokens, masked_pos, is_next]\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        sent_a_index, sent_b_index= random.randrange(len(tokens_list)), random.randrange(len(tokens_list))\n",
    "        if random.random() < 0.5: # 以50%的概率get postive, since getting a negative sample has a much higher prob.\n",
    "            sent_b_index = (sent_a_index + 1) % len(tokens_list) # a_index could be the last index -> out of bound\n",
    "        if sent_b_index == sent_a_index + 1: # positive sample\n",
    "            if positive == batch_size/2:\n",
    "                continue\n",
    "        else: # negative sample\n",
    "            if negative == batch_size/2:\n",
    "                continue\n",
    "\n",
    "        tokens_a, tokens_b= tokens_list[sent_a_index], tokens_list[sent_b_index]\n",
    "\n",
    "        input_ids = [word_to_id['[CLS]']] + tokens_a + [word_to_id['[SEP]']] + tokens_b + [word_to_id['[SEP]']]\n",
    "        segment_ids = [1] * (1 + len(tokens_a) + 1) + [2] * (len(tokens_b) + 1)\n",
    "\n",
    "        # The following code is used for the Masked Language Modeling (MLM) task.\n",
    "        n_pred =  min(MAX_PRED, max(1, int(round(len(input_ids) * 0.15)))) # Predict at most 15 % of tokens in one sentence\n",
    "        masked_candidates_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word_to_id['[CLS]'] and token != word_to_id['[SEP]']]\n",
    "        random.shuffle(masked_candidates_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in masked_candidates_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            ### START YOUR CODE ###\n",
    "            # Throw a dice to decide if you want to replace the token with [MASK], random word, or remain the same\n",
    "            if random.random() < 0.8:\n",
    "                input_ids[pos] = word_to_id['[MASK]']\n",
    "            elif random.random() < 0.5:\n",
    "                input_ids[pos] = random.randint(4, VOCAB_SIZE - 1)\n",
    "            ### END YOUR CODE ###\n",
    "\n",
    "        # Make zero paddings\n",
    "        n_pad = MAX_LEN - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero padding (remaining 85%) of the tokens\n",
    "        if MAX_PRED > n_pred:\n",
    "            n_pad = MAX_PRED - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        # The following code is used for the Next Sentence Prediction (NSP) task.\n",
    "        ### START YOUR CODE ###\n",
    "        # Decide if the is_next label is positive or negative, by comparing sent_a_index and sent_b_index\n",
    "        # Don't forget to increment the positive/negative count\n",
    "        if sent_b_index == sent_a_index + 1:\n",
    "            is_next = 1\n",
    "            positive += 1\n",
    "        else:\n",
    "            is_next = 0\n",
    "            negative += 1\n",
    "        batch.append([input_ids, segment_ids, masked_tokens, masked_pos, is_next])\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "random.seed(0)\n",
    "batch = make_batch(tokens_list, batch_size, word2id)\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost = 23.359852\n",
      "Epoch: 0100 cost = 18.796139\n",
      "Epoch: 0150 cost = 13.363510\n",
      "Epoch: 0200 cost = 15.341948\n",
      "Epoch: 0250 cost = 13.096960\n",
      "Epoch: 0300 cost = 12.453255\n",
      "Epoch: 0350 cost = 11.351063\n",
      "Epoch: 0400 cost = 9.280038\n",
      "Epoch: 0450 cost = 14.212572\n",
      "Epoch: 0500 cost = 9.653446\n",
      "Epoch: 0550 cost = 11.246633\n",
      "Epoch: 0600 cost = 8.388114\n",
      "Epoch: 0650 cost = 7.744303\n",
      "Epoch: 0700 cost = 12.893216\n",
      "Epoch: 0750 cost = 8.771959\n",
      "Epoch: 0800 cost = 7.355745\n",
      "Epoch: 0850 cost = 9.224614\n",
      "Epoch: 0900 cost = 9.108873\n",
      "Epoch: 0950 cost = 7.613592\n",
      "Epoch: 1000 cost = 6.713401\n",
      "Epoch: 1050 cost = 14.162107\n",
      "Epoch: 1100 cost = 7.889677\n",
      "Epoch: 1150 cost = 10.366699\n",
      "Epoch: 1200 cost = 8.378750\n",
      "Epoch: 1250 cost = 6.267869\n",
      "Epoch: 1300 cost = 7.112204\n",
      "Epoch: 1350 cost = 5.155307\n",
      "Epoch: 1400 cost = 9.321204\n",
      "Epoch: 1450 cost = 5.851353\n",
      "Epoch: 1500 cost = 6.361363\n"
     ]
    }
   ],
   "source": [
    "epochs = 1500\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = BERT(VOCAB_SIZE, MAX_LEN).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) # You can also try two separate losses for each task\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "# batch = make_batch(tokens_list, batch_size, word_to_id)\n",
    "# input_ids, segment_ids, masked_tokens, masked_pos, is_next = map(torch.LongTensor, zip(*batch))\n",
    "# input_ids.to(device)\n",
    "# segment_ids.to(device)\n",
    "# masked_tokens.to(device)\n",
    "# masked_pos.to(device)\n",
    "# is_next.to(device)\n",
    "for epoch in range(epochs):\n",
    "    batch = make_batch(tokens_list, batch_size, word2id)\n",
    "    input_ids, segment_ids, masked_tokens, masked_pos, is_next = map(torch.LongTensor, zip(*batch))\n",
    "    input_ids = input_ids.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    masked_tokens = masked_tokens.to(device)\n",
    "    masked_pos = masked_pos.to(device)\n",
    "    is_next = is_next.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    logits_lm, logits_clsf = model.forward(input_ids, segment_ids, masked_pos)\n",
    "    # Hint: Check the shape of logits_lm and decide if post-processing is needed\n",
    "    loss_lm = criterion(logits_lm.view(-1, logits_lm.size(-1)), masked_tokens.view(-1))\n",
    "    loss_lm = loss_lm.mean()\n",
    "    # loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)\n",
    "    loss_clsf = criterion(logits_clsf, is_next)\n",
    "    loss = loss_lm + loss_clsf\n",
    "    # loss = criterion(logits_lm.view(-1, logits_lm.size(-1)), masked_tokens.view(-1))+ criterion(logits_clsf, is_next)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(test_pair_path, 'r') as f:\n",
    "    test_pairs = f.readlines()\n",
    "len(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS] 其 [MASK] 本 臃 肿 而 不 中 绳 墨 ， 其 [MASK] [MASK] 卷 曲 而 不 中 规 矩 。 [SEP] 立 之 涂 ， [MASK] 者 不 顾 。 [SEP]',\n",
       " '1',\n",
       " '大 小 枝 匠\\n')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence, is_next, masked_tokens = test_pairs[0].split('\\t')\n",
    "sentence, is_next, masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 10)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = [word2id[w] for w in sentence.split()]\n",
    "sep1_idx = input_ids.index(word2id['[SEP]'])\n",
    "#sen1,sen2: 24, 10\n",
    "segment_ids = [1] * (sep1_idx + 1) + [2] * (len(input_ids) - sep1_idx - 1)\n",
    "cnt1 = 0\n",
    "cnt2 = 0\n",
    "for i in segment_ids:\n",
    "    if i ==1:\n",
    "        cnt1 += 1\n",
    "    else:\n",
    "        cnt2 += 1\n",
    "cnt1, cnt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 13, 14, 28], [929, 175, 353, 84])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token == word2id['[MASK]']]\n",
    "\n",
    "masked_tokens_id = [word2id[token] for token in masked_tokens.split()]\n",
    "masked_pos, masked_tokens_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_next = int(is_next)\n",
    "input_ids, segment_ids, masked_tokens_id, masked_pos, is_next = map(torch.LongTensor,\n",
    "                                                                zip((input_ids, segment_ids, masked_tokens_id, masked_pos, [is_next])))\n",
    "input_ids = input_ids.to(device)\n",
    "segment_ids = segment_ids.to(device)\n",
    "masked_tokens_id = masked_tokens_id.to(device)\n",
    "masked_pos = masked_pos.to(device)\n",
    "is_next = is_next.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1066,  495,  787,  495], device='cuda:0'),\n",
       " tensor([[929, 175, 353,  84]], device='cuda:0'),\n",
       " tensor(1, device='cuda:0'))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_lm, logits_clsf = model.forward(input_ids, segment_ids, masked_pos)\n",
    "\n",
    "predicted_ids = logits_lm.argmax(dim=2).squeeze()\n",
    "predicted_next = logits_clsf.argmax(dim=-1).squeeze()\n",
    "predicted_ids, masked_tokens_id, predicted_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_ids == masked_tokens_id).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM Accuracy: 0.03724928366762178\n",
      "NSP Accuracy: 0.5087719298245614\n"
     ]
    }
   ],
   "source": [
    "mlm_total = 0\n",
    "mlm_correct = 0\n",
    "nsp_total = 0\n",
    "nsp_correct = 0\n",
    "for data in test_pairs:\n",
    "    sentence, is_next, masked_tokens = data.split('\\t')\n",
    "    input_ids = [word2id[w] for w in sentence.split()]\n",
    "    sep1_idx = input_ids.index(word2id['[SEP]'])\n",
    "    segment_ids = [1] * (sep1_idx + 1) + [2] * (len(input_ids) - sep1_idx - 1)\n",
    "    masked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token == word2id['[MASK]']]\n",
    "    masked_tokens_id = [word2id[token] for token in masked_tokens.split()]\n",
    "    is_next = int(is_next)\n",
    "    input_ids, segment_ids, masked_tokens_id, masked_pos, is_next = map(torch.LongTensor,\n",
    "                                                                zip((input_ids, segment_ids, masked_tokens_id, masked_pos, [is_next])))\n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    masked_tokens_id = masked_tokens_id.to(device)\n",
    "    masked_pos = masked_pos.to(device)\n",
    "    is_next = is_next.to(device)\n",
    "    \n",
    "    logits_lm, logits_clsf = model.forward(input_ids, segment_ids, masked_pos)\n",
    "    # NSP\n",
    "    predicted_next = logits_clsf.argmax(dim=-1).squeeze()\n",
    "    nsp_total += 1\n",
    "    nsp_correct += (predicted_next == is_next).sum().item()\n",
    "\n",
    "    # MLM\n",
    "    # predicted_ids = logits_lm.argmax(dim=2).squeeze()\n",
    "    predicted_ids = torch.argmax(logits_lm, dim=2)\n",
    "    mlm_total += len(masked_tokens)\n",
    "    mlm_correct += (predicted_ids == masked_tokens_id).sum().item()\n",
    "\n",
    "print('MLM Accuracy:', mlm_correct/mlm_total)\n",
    "print('NSP Accuracy:', nsp_correct/nsp_total)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
