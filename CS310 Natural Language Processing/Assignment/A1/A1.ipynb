{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1. Neural Text Classification\n",
    "## CS310 Natural Language Processing\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "You should roughtly follow the structure of the notebook. Add additional cells if you feel needed. \n",
    "\n",
    "You can (and you should) re-use the code from Lab 2. \n",
    "\n",
    "Make sure your code is readable and well-structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "train_data_path = 'train.jsonl'\n",
    "test_data_path = 'test.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12677, 651)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "with open(train_data_path, 'r') as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        train_data.append((record['sentence'], record['label'][0]))\n",
    "\n",
    "with open(test_data_path, 'r') as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        test_data.append((record['sentence'], record['label'][0]))\n",
    "        \n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('卖油条小刘说：我说', 0), ('保姆小张说：干啥子嘛？', 0), ('卖油条小刘说：你看你往星空看月朦胧，鸟朦胧', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 根据索引获取一个样本，并返回 (text, label) 的元组\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_data)\n",
    "test_dataset = MyDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('卖油条小刘说：我说', 0), ('保姆小张说：干啥子嘛？', 0), ('卖油条小刘说：你看你往星空看月朦胧，鸟朦胧', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Character as a Token, Discard Others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def chinese_tokenizer(text): # use unicode range \\u4e00 - \\u9fa5\n",
    "    return [char for char in text if '\\u4e00' <= char <= '\\u9fa5'] \n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        tokens = chinese_tokenizer(text)  # 对句子进行分词\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('卖油条小刘说：我说', 0)\n",
      "('保姆小张说：干啥子嘛？', 0)\n",
      "['卖', '油', '条', '小', '刘', '说', '我', '说']\n",
      "['保', '姆', '小', '张', '说', '干', '啥', '子', '嘛']\n",
      "['卖', '油', '条', '小', '刘', '说', '你', '看', '你', '往', '星', '空', '看', '月', '朦', '胧', '鸟', '朦', '胧']\n",
      "['卖', '油', '条', '小', '刘', '说', '咱', '是', '不', '是', '歇', '一', '下', '这', '双', '疲', '惫', '的', '双', '腿']\n",
      "['卖', '油', '条', '小', '刘', '说', '快', '把', '我', '累', '死', '了']\n",
      "['卖', '油', '条', '小', '刘', '说', '我', '说', '亲', '爱', '的', '大', '姐', '你', '贵', '姓', '啊']\n",
      "['保', '姆', '小', '张', '说', '我', '免', '贵', '姓', '张', '我', '叫', '张', '凤', '姑']\n",
      "['卖', '油', '条', '小', '刘', '说', '凤', '姑']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "print(train_dataset[0])\n",
    "print(train_dataset[1])\n",
    "\n",
    "for tokens in yield_tokens(train_dataset): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer with Chinese/English/Number/Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['卖', '油', '条', '小', '刘', '说', '我', '说', '块', '钱', '你', '好', '哇', '123', 'this', 'is', 'an', 'example', 'of', 'text', '：', '！', '。']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def improved_chinese_tokenizer(text):\n",
    "    tokens = []\n",
    "\n",
    "    digit = re.compile(r'\\d+')\n",
    "    english = re.compile(r'[a-zA-Z]+')\n",
    "    punctuation = re.compile(r'[。|？|，|！|：|；|“|”]')\n",
    "\n",
    "    # Chinese characters\n",
    "    chinese_tokens = [char for char in text if '\\u4e00' <= char <= '\\u9fa5']\n",
    "    tokens.extend(chinese_tokens)\n",
    "\n",
    "    for match in digit.finditer(text):\n",
    "        tokens.append(match.group())\n",
    "\n",
    "    for match in english.finditer(text):\n",
    "        tokens.append(match.group())\n",
    "\n",
    "    for match in punctuation.finditer(text):\n",
    "        tokens.append(match.group())\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Example usage:\n",
    "text = \"卖油条小刘说：我说123块钱 你好！this is an example of text.哇。\"\n",
    "tokens = improved_chinese_tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With consecutive digits, English words, and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        tokens = improved_chinese_tokenizer(text)\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('卖油条小刘说：我说', 0)\n",
      "('保姆小张说：干啥子嘛？', 0)\n",
      "['卖', '油', '条', '小', '刘', '说', '我', '说', '：']\n",
      "['保', '姆', '小', '张', '说', '干', '啥', '子', '嘛', '：', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '你', '看', '你', '往', '星', '空', '看', '月', '朦', '胧', '鸟', '朦', '胧', '：', '，']\n",
      "['卖', '油', '条', '小', '刘', '说', '咱', '是', '不', '是', '歇', '一', '下', '这', '双', '疲', '惫', '的', '双', '腿', '：', '，', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '快', '把', '我', '累', '死', '了', '：']\n",
      "['卖', '油', '条', '小', '刘', '说', '我', '说', '亲', '爱', '的', '大', '姐', '你', '贵', '姓', '啊', '：', '？']\n",
      "['保', '姆', '小', '张', '说', '我', '免', '贵', '姓', '张', '我', '叫', '张', '凤', '姑', '：']\n",
      "['卖', '油', '条', '小', '刘', '说', '凤', '姑', '：']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "print(train_data[0])\n",
    "print(train_data[1])\n",
    "\n",
    "for tokens in improved_yield_tokens(iter(train_data)): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2687\n",
      "[3, 31, 402, 496, 0]\n",
      "Vocabulary size: 2806\n",
      "[4, 34, 407, 501, 69, 301, 0]\n",
      "[473, 460, 282, 894, 69, 301, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vocab1 = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\"])\n",
    "vocab1.set_default_index(vocab1[\"<unk>\"])\n",
    "\n",
    "vocab = build_vocab_from_iterator(improved_yield_tokens(train_dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab1))\n",
    "print(vocab1(['你', '好', '世','界', '!']))\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(vocab(['你', '好', '世','界', '！','。','@']))\n",
    "print(vocab(['卖', '油', '条','鸡', '！','。','@','123']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(improved_chinese_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 34, 407, 501, 360, 360, 69, 301]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Test text_pipeline()\n",
    "tokens = text_pipeline('你好世界！哈哈。')\n",
    "print(tokens)\n",
    "\n",
    "# Test label_pipeline()\n",
    "lbl = label_pipeline('1')\n",
    "print(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Batch\n",
    "\n",
    "Define the `Collate_batch` function, which will be used to process the \"raw\" data batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# The operator 'aten::_embedding_bag' is not currently implemented for the MPS device.\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for _text, _label in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        token_ids = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0))  # 将每个样本的token数量添加到offsets列表中\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    # 计算偏移量的累积和，从而得到每个批次数据在合并后的张量中的起始位置\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    token_ids = torch.cat(token_ids_list)  # 得到一个包含所有样本的token IDs的tensor\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = train_dataset\n",
    "# Use collate_batch to generate the dataloader\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 label: tensor([0, 0, 1, 0, 0, 1, 0, 0])\n",
      "batch 0 text: tensor([ 473,  460,  282,   23,  423,    1,    3,    1,    2,   73,   83,   23,\n",
      "         113,    1,   98,  483,   46,   59,    2,   33,  473,  460,  282,   23,\n",
      "         423,    1,    4,   55,    4,  305,  760,  869,   55,  494, 2131, 2210,\n",
      "        1214, 2131, 2210,    2,    6,  473,  460,  282,   23,  423,    1,   71,\n",
      "           7,    5,    7,  906,   18,   75,   10,  875, 2181, 2484,    8,  875,\n",
      "        1130,    2,    6,   33,  473,  460,  282,   23,  423,    1,  187,   86,\n",
      "           3,  610,  183,    9,    2,  473,  460,  282,   23,  423,    1,    3,\n",
      "           1,  308,  164,    8,   32,  141,    4,  687,  453,   21,    2,   33,\n",
      "          73,   83,   23,  113,    1,    3,  819,  687,  453,  113,    3,  149,\n",
      "         113, 1183,  221,    2,  473,  460,  282,   23,  423,    1, 1183,  221,\n",
      "           2])\n",
      "batch 0 offsets: tensor([  0,   9,  20,  41,  64,  77,  96, 112])\n",
      "Number of tokens:  121\n",
      "Number of examples in one batch:  8\n",
      "Example 1:  tensor([473, 460, 282,  23, 423,   1,   3,   1,   2])\n",
      "Example 7:  tensor([  73,   83,   23,  113,    1,    3,  819,  687,  453,  113,    3,  149,\n",
      "         113, 1183,  221,    2])\n"
     ]
    }
   ],
   "source": [
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "    print(f\"batch {i} label: {labels}\")\n",
    "    print(f\"batch {i} text: {token_ids}\")\n",
    "    print(f\"batch {i} offsets: {offsets}\")\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "print('Example 1: ', token_ids[offsets[0]:offsets[1]])\n",
    "print('Example 7: ', token_ids[offsets[6]:offsets[7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, hidden_dim1, hidden_dim2):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim2, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        for layer in self.hidden_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.uniform_(-initrange, initrange)\n",
    "                layer.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        x = self.hidden_layers(embedded)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2806)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model\n",
    "train_iter = iter(train_dataset)\n",
    "num_class = len(set([label for (_, label) in train_iter])) # binary classification here\n",
    "vocab_size = len(vocab)\n",
    "num_class, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding size\n",
    "emsize =  64\n",
    "\n",
    "hidden_dim1 = 16\n",
    "hidden_dim2 = 8\n",
    "# dropout_prob = 0\n",
    "\n",
    "\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class, hidden_dim1, hidden_dim2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([8, 2])\n",
      "output: tensor([[ 0.1136, -0.1126],\n",
      "        [ 0.1427, -0.2067],\n",
      "        [ 0.0157, -0.0216],\n",
      "        [ 0.1239, -0.1515],\n",
      "        [-0.1071, -0.2682],\n",
      "        [ 0.0871, -0.1255],\n",
      "        [ 0.0756, -0.1000],\n",
      "        [ 0.0686, -0.0870]])\n",
      "TextClassificationModel(\n",
      "  (embedding): EmbeddingBag(2806, 64, mode='mean')\n",
      "  (hidden_layers): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (fc): Linear(in_features=8, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "# Examine the output\n",
    "print('output size:', output.size())\n",
    "print('output:', output)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score,precision_recall_fscore_support\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epoch: int): # criterion: loss function\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 200 # print log every 200 batches\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(token_ids, offsets)\n",
    "        try:\n",
    "            loss = criterion(output, labels)\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            # print('token_ids: ', token_ids)\n",
    "            # print('offsets: ', offsets)\n",
    "            raise\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # 裁剪，防止梯度爆炸。确保梯度的范数不超过给定的阈值（在这里是0.1\n",
    "        # 如果梯度的范数超过了阈值，那么梯度将按比例缩放，以使其范数不超过指定的阈值。防止梯度过大导致的参数更新过大而影响训练效果。\n",
    "        optimizer.step() # update the parameters using the gradients\n",
    "\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_correct, total_count = 0, 0\n",
    "    y_labels = []\n",
    "    y_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            output = model(text, offsets)\n",
    "            predictions = output.argmax(1)\n",
    "\n",
    "            total_correct += (predictions == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "            y_labels.extend(label.tolist())\n",
    "            y_preds.extend(predictions.tolist())\n",
    "\n",
    "    accuracy = total_correct / total_count\n",
    "    precision, recall, f1Score, _ = precision_recall_fscore_support(y_labels, y_preds, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1Score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters, loss, optimizer, and learning-rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1) # decay lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test `criterion`, i.e., the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6307)\n",
      "loss non-reduced: tensor([0.5865, 0.5337, 0.7120, 0.5649, 0.6159, 0.8050, 0.6092, 0.6184])\n",
      "mean of loss non-reduced: tensor(0.6307)\n",
      "loss manually computed: tensor(0.5865)\n"
     ]
    }
   ],
   "source": [
    "# First, obtain some output and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "# keep multiple losses for all samples in a batch, not just the mean\n",
    "criterion2 = torch.nn.CrossEntropyLoss(reduction='none') \n",
    "\n",
    "loss2 = criterion2(output, labels)\n",
    "print('loss non-reduced:', loss2)\n",
    "print('mean of loss non-reduced:', torch.mean(loss2))\n",
    "\n",
    "# Manually calculate the loss\n",
    "probs = torch.exp(output[0,:]) / torch.exp(output[0,:]).sum()\n",
    "loss3 = -torch.log(probs[labels[0]])\n",
    "print('loss manually computed:', loss3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare train, valid, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train, valid, and test data\n",
    "train_iter = MyDataset(train_data)\n",
    "test_iter = MyDataset(test_data)\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1506 batches | accuracy    0.711\n",
      "| epoch   1 |   400/ 1506 batches | accuracy    0.676\n",
      "| epoch   1 |   600/ 1506 batches | accuracy    0.708\n",
      "| epoch   1 |   800/ 1506 batches | accuracy    0.699\n",
      "| epoch   1 |  1000/ 1506 batches | accuracy    0.702\n",
      "| epoch   1 |  1200/ 1506 batches | accuracy    0.716\n",
      "| epoch   1 |  1400/ 1506 batches | accuracy    0.685\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  1.25s | valid accuracy    0.472 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   200/ 1506 batches | accuracy    0.695\n",
      "| epoch   2 |   400/ 1506 batches | accuracy    0.714\n",
      "| epoch   2 |   600/ 1506 batches | accuracy    0.688\n",
      "| epoch   2 |   800/ 1506 batches | accuracy    0.715\n",
      "| epoch   2 |  1000/ 1506 batches | accuracy    0.669\n",
      "| epoch   2 |  1200/ 1506 batches | accuracy    0.708\n",
      "| epoch   2 |  1400/ 1506 batches | accuracy    0.693\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  1.21s | valid accuracy    0.692 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 |   200/ 1506 batches | accuracy    0.692\n",
      "| epoch   3 |   400/ 1506 batches | accuracy    0.684\n",
      "| epoch   3 |   600/ 1506 batches | accuracy    0.680\n",
      "| epoch   3 |   800/ 1506 batches | accuracy    0.691\n",
      "| epoch   3 |  1000/ 1506 batches | accuracy    0.701\n",
      "| epoch   3 |  1200/ 1506 batches | accuracy    0.677\n",
      "| epoch   3 |  1400/ 1506 batches | accuracy    0.696\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  1.28s | valid accuracy    0.692 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 |   200/ 1506 batches | accuracy    0.690\n",
      "| epoch   4 |   400/ 1506 batches | accuracy    0.691\n",
      "| epoch   4 |   600/ 1506 batches | accuracy    0.661\n",
      "| epoch   4 |   800/ 1506 batches | accuracy    0.693\n",
      "| epoch   4 |  1000/ 1506 batches | accuracy    0.716\n",
      "| epoch   4 |  1200/ 1506 batches | accuracy    0.701\n",
      "| epoch   4 |  1400/ 1506 batches | accuracy    0.693\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  1.23s | valid accuracy    0.692 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   5 |   200/ 1506 batches | accuracy    0.692\n",
      "| epoch   5 |   400/ 1506 batches | accuracy    0.664\n",
      "| epoch   5 |   600/ 1506 batches | accuracy    0.696\n",
      "| epoch   5 |   800/ 1506 batches | accuracy    0.705\n",
      "| epoch   5 |  1000/ 1506 batches | accuracy    0.700\n",
      "| epoch   5 |  1200/ 1506 batches | accuracy    0.696\n",
      "| epoch   5 |  1400/ 1506 batches | accuracy    0.690\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  1.21s | valid accuracy    0.692 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   6 |   200/ 1506 batches | accuracy    0.698\n",
      "| epoch   6 |   400/ 1506 batches | accuracy    0.698\n",
      "| epoch   6 |   600/ 1506 batches | accuracy    0.693\n",
      "| epoch   6 |   800/ 1506 batches | accuracy    0.694\n",
      "| epoch   6 |  1000/ 1506 batches | accuracy    0.711\n",
      "| epoch   6 |  1200/ 1506 batches | accuracy    0.681\n",
      "| epoch   6 |  1400/ 1506 batches | accuracy    0.686\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  1.21s | valid accuracy    0.692 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 |   200/ 1506 batches | accuracy    0.689\n",
      "| epoch   7 |   400/ 1506 batches | accuracy    0.677\n",
      "| epoch   7 |   600/ 1506 batches | accuracy    0.684\n",
      "| epoch   7 |   800/ 1506 batches | accuracy    0.688\n",
      "| epoch   7 |  1000/ 1506 batches | accuracy    0.693\n",
      "| epoch   7 |  1200/ 1506 batches | accuracy    0.685\n",
      "| epoch   7 |  1400/ 1506 batches | accuracy    0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  1.34s | valid accuracy    0.692 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   200/ 1506 batches | accuracy    0.705\n",
      "| epoch   8 |   400/ 1506 batches | accuracy    0.711\n",
      "| epoch   8 |   600/ 1506 batches | accuracy    0.685\n",
      "| epoch   8 |   800/ 1506 batches | accuracy    0.686\n",
      "| epoch   8 |  1000/ 1506 batches | accuracy    0.671\n",
      "| epoch   8 |  1200/ 1506 batches | accuracy    0.682\n",
      "| epoch   8 |  1400/ 1506 batches | accuracy    0.693\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  1.30s | valid accuracy    0.692 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   9 |   200/ 1506 batches | accuracy    0.694\n",
      "| epoch   9 |   400/ 1506 batches | accuracy    0.701\n",
      "| epoch   9 |   600/ 1506 batches | accuracy    0.692\n",
      "| epoch   9 |   800/ 1506 batches | accuracy    0.684\n",
      "| epoch   9 |  1000/ 1506 batches | accuracy    0.692\n",
      "| epoch   9 |  1200/ 1506 batches | accuracy    0.710\n",
      "| epoch   9 |  1400/ 1506 batches | accuracy    0.689\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  1.22s | valid accuracy    0.644 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   200/ 1506 batches | accuracy    0.702\n",
      "| epoch  10 |   400/ 1506 batches | accuracy    0.698\n",
      "| epoch  10 |   600/ 1506 batches | accuracy    0.714\n",
      "| epoch  10 |   800/ 1506 batches | accuracy    0.732\n",
      "| epoch  10 |  1000/ 1506 batches | accuracy    0.723\n",
      "| epoch  10 |  1200/ 1506 batches | accuracy    0.703\n",
      "| epoch  10 |  1400/ 1506 batches | accuracy    0.701\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  1.24s | valid accuracy    0.666 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    # accu_val = evaluate(model, valid_dataloader, criterion)\n",
    "    accu_val, precision, recall, f1 = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "        # print('')\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"A1_text_classification_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate with Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy    0.696, precision    0.673, recall    0.696, f1_score    0.682\n"
     ]
    }
   ],
   "source": [
    "# accu_test = evaluate(model, test_dataloader, criterion)\n",
    "accu_val, precision, recall, f1_score = evaluate(model, test_dataloader, criterion)\n",
    "print(\"test accuracy {:8.3f}, precision {:8.3f}, recall {:8.3f}, f1_score {:8.3f}\".format(accu_val, precision, recall, f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12677, 651)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('卖油条小刘说：我说', 0), ('保姆小张说：干啥子嘛？', 0), ('卖油条小刘说：你看你往星空看月朦胧，鸟朦胧', 1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = MyDataset(train_data)\n",
    "test_dataset = MyDataset(test_data)\n",
    "train_dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/zs/c32hzq1j6t54vw2q044ggrsr0000gn/T/jieba.cache\n",
      "Loading model cost 0.387 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '是', '南方', '科技', '大学', '的', '计算机系', '学生', '。']\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.lcut(\"我是南方科技大学的计算机系学生。\")  # 默认是精确模式\n",
    "print(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_tokenizer(text):\n",
    "    return jieba.lcut(text)\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        tokens = jieba_tokenizer(text)  # 对句子进行分词\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('卖油条小刘说：我说', 0)\n",
      "('保姆小张说：干啥子嘛？', 0)\n",
      "['卖', '油条', '小', '刘说', '：', '我', '说']\n",
      "['保姆', '小张', '说', '：', '干', '啥子', '嘛', '？']\n",
      "['卖', '油条', '小', '刘说', '：', '你', '看', '你', '往', '星空', '看', '月', '朦胧', '，', '鸟', '朦胧']\n",
      "['卖', '油条', '小', '刘说', '：', '咱', '是不是', '歇', '一下', '这', '双', '，', '疲惫', '的', '双腿', '？']\n",
      "['卖', '油条', '小', '刘说', '：', '快', '把', '我', '累死', '了']\n",
      "['卖', '油条', '小', '刘说', '：', '我', '说', '亲爱', '的', '大姐', '你', '贵姓', '啊', '？']\n",
      "['保姆', '小张', '说', '：', '我免', '贵姓', '张', '我', '叫', '张凤姑']\n",
      "['卖', '油条', '小', '刘说', '：', '凤姑']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "print(train_data[0])\n",
    "print(train_data[1])\n",
    "\n",
    "for tokens in yield_tokens(iter(train_data)): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13847\n",
      "[5, 48, 515, 0, 43, 153, 0]\n",
      "[385, 3516, 2129, 6008, 43, 153, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(vocab(['你', '好', '世界','界', '！','。','@']))\n",
    "print(vocab(['卖', '油', '条','鸡', '！','。','@','123']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(jieba_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[561, 515, 43, 938, 153, 3, 12, 385, 536, 6]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Test text_pipeline()\n",
    "tokens = text_pipeline('你好世界！哈哈。我是卖油条的')\n",
    "print(tokens)\n",
    "\n",
    "# Test label_pipeline()\n",
    "lbl = label_pipeline('1')\n",
    "print(lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Batch\n",
    "\n",
    "Define the `Collate_batch` function, which will be used to process the \"raw\" data batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Use collate_batch to generate the dataloader\n",
    "train_dataset = MyDataset(train_data)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# The operator 'aten::_embedding_bag' is not currently implemented for the MPS device.\n",
    "\n",
    "train_iter = train_dataset\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 13847)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model\n",
    "train_iter = iter(train_dataset)\n",
    "num_class = len(set([label for (_, label) in train_iter])) # binary classification here\n",
    "vocab_size = len(vocab)\n",
    "num_class, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_dim1 = 16\n",
    "hidden_dim2 = 8\n",
    "\n",
    "# embedding size\n",
    "emsize =  64\n",
    "\n",
    "model1 = TextClassificationModel(vocab_size, emsize, num_class, hidden_dim1, hidden_dim2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters, loss, optimizer, and learning-rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1) # decay lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train, valid, and test data\n",
    "train_iter = MyDataset(train_data)\n",
    "test_iter = MyDataset(test_data)\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1506 batches | accuracy    0.700\n",
      "| epoch   1 |   400/ 1506 batches | accuracy    0.703\n",
      "| epoch   1 |   600/ 1506 batches | accuracy    0.702\n",
      "| epoch   1 |   800/ 1506 batches | accuracy    0.667\n",
      "| epoch   1 |  1000/ 1506 batches | accuracy    0.701\n",
      "| epoch   1 |  1200/ 1506 batches | accuracy    0.700\n",
      "| epoch   1 |  1400/ 1506 batches | accuracy    0.679\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  2.74s | valid accuracy    0.333 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   200/ 1506 batches | accuracy    0.682\n",
      "| epoch   2 |   400/ 1506 batches | accuracy    0.693\n",
      "| epoch   2 |   600/ 1506 batches | accuracy    0.685\n",
      "| epoch   2 |   800/ 1506 batches | accuracy    0.694\n",
      "| epoch   2 |  1000/ 1506 batches | accuracy    0.731\n",
      "| epoch   2 |  1200/ 1506 batches | accuracy    0.681\n",
      "| epoch   2 |  1400/ 1506 batches | accuracy    0.661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  2.81s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   200/ 1506 batches | accuracy    0.662\n",
      "| epoch   3 |   400/ 1506 batches | accuracy    0.713\n",
      "| epoch   3 |   600/ 1506 batches | accuracy    0.699\n",
      "| epoch   3 |   800/ 1506 batches | accuracy    0.662\n",
      "| epoch   3 |  1000/ 1506 batches | accuracy    0.705\n",
      "| epoch   3 |  1200/ 1506 batches | accuracy    0.698\n",
      "| epoch   3 |  1400/ 1506 batches | accuracy    0.710\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  2.79s | valid accuracy    0.621 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   200/ 1506 batches | accuracy    0.711\n",
      "| epoch   4 |   400/ 1506 batches | accuracy    0.701\n",
      "| epoch   4 |   600/ 1506 batches | accuracy    0.713\n",
      "| epoch   4 |   800/ 1506 batches | accuracy    0.717\n",
      "| epoch   4 |  1000/ 1506 batches | accuracy    0.724\n",
      "| epoch   4 |  1200/ 1506 batches | accuracy    0.713\n",
      "| epoch   4 |  1400/ 1506 batches | accuracy    0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  2.70s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   200/ 1506 batches | accuracy    0.701\n",
      "| epoch   5 |   400/ 1506 batches | accuracy    0.712\n",
      "| epoch   5 |   600/ 1506 batches | accuracy    0.715\n",
      "| epoch   5 |   800/ 1506 batches | accuracy    0.702\n",
      "| epoch   5 |  1000/ 1506 batches | accuracy    0.695\n",
      "| epoch   5 |  1200/ 1506 batches | accuracy    0.699\n",
      "| epoch   5 |  1400/ 1506 batches | accuracy    0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  2.76s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   200/ 1506 batches | accuracy    0.697\n",
      "| epoch   6 |   400/ 1506 batches | accuracy    0.725\n",
      "| epoch   6 |   600/ 1506 batches | accuracy    0.728\n",
      "| epoch   6 |   800/ 1506 batches | accuracy    0.711\n",
      "| epoch   6 |  1000/ 1506 batches | accuracy    0.689\n",
      "| epoch   6 |  1200/ 1506 batches | accuracy    0.714\n",
      "| epoch   6 |  1400/ 1506 batches | accuracy    0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  2.70s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   200/ 1506 batches | accuracy    0.727\n",
      "| epoch   7 |   400/ 1506 batches | accuracy    0.706\n",
      "| epoch   7 |   600/ 1506 batches | accuracy    0.711\n",
      "| epoch   7 |   800/ 1506 batches | accuracy    0.703\n",
      "| epoch   7 |  1000/ 1506 batches | accuracy    0.704\n",
      "| epoch   7 |  1200/ 1506 batches | accuracy    0.713\n",
      "| epoch   7 |  1400/ 1506 batches | accuracy    0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  2.73s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   200/ 1506 batches | accuracy    0.700\n",
      "| epoch   8 |   400/ 1506 batches | accuracy    0.712\n",
      "| epoch   8 |   600/ 1506 batches | accuracy    0.719\n",
      "| epoch   8 |   800/ 1506 batches | accuracy    0.713\n",
      "| epoch   8 |  1000/ 1506 batches | accuracy    0.732\n",
      "| epoch   8 |  1200/ 1506 batches | accuracy    0.704\n",
      "| epoch   8 |  1400/ 1506 batches | accuracy    0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooperz/opt/anaconda3/envs/nlp/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  2.77s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   200/ 1506 batches | accuracy    0.693\n",
      "| epoch   9 |   400/ 1506 batches | accuracy    0.706\n",
      "| epoch   9 |   600/ 1506 batches | accuracy    0.734\n",
      "| epoch   9 |   800/ 1506 batches | accuracy    0.738\n",
      "| epoch   9 |  1000/ 1506 batches | accuracy    0.701\n",
      "| epoch   9 |  1200/ 1506 batches | accuracy    0.701\n",
      "| epoch   9 |  1400/ 1506 batches | accuracy    0.699\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  2.75s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   200/ 1506 batches | accuracy    0.713\n",
      "| epoch  10 |   400/ 1506 batches | accuracy    0.718\n",
      "| epoch  10 |   600/ 1506 batches | accuracy    0.714\n",
      "| epoch  10 |   800/ 1506 batches | accuracy    0.707\n",
      "| epoch  10 |  1000/ 1506 batches | accuracy    0.706\n",
      "| epoch  10 |  1200/ 1506 batches | accuracy    0.736\n",
      "| epoch  10 |  1400/ 1506 batches | accuracy    0.694\n",
      "\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  2.80s | valid accuracy    0.688 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model1, train_dataloader, optimizer, criterion, epoch)\n",
    "    # accu_val = evaluate(model1, valid_dataloader, criterion)\n",
    "    accu_val, precision, recall, f1 = evaluate(model1, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "        print('')\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model1.state_dict(), \"A1_jieba_text_classification_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy    0.716, precision    0.694, recall    0.716, f1_score    0.682\n"
     ]
    }
   ],
   "source": [
    "accu_val, precision, recall, f1 = evaluate(model1, test_dataloader, criterion)\n",
    "print(\"test accuracy {:8.3f}, precision {:8.3f}, recall {:8.3f}, f1_score {:8.3f}\".format(accu_val, precision, recall, f1_score))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
